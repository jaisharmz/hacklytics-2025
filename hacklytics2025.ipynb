{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "82AXI8OlU8kB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "# class TransformerAutoencoder(nn.Module):\n",
        "#     def __init__(self, time_steps, d_model=128, nhead=4, num_layers=3, latent_dim=64, dim_feedforward=256, dropout=0.1):\n",
        "#         super().__init__()\n",
        "#         self.input_proj = nn.Linear(1, d_model)  # Project feature dim (1) to d_model\n",
        "#         self.pos_encoder = PositionalEncoding(d_model, max_len=time_steps)\n",
        "\n",
        "#         # Transformer Encoder\n",
        "#         encoder_layer = nn.TransformerEncoderLayer(\n",
        "#             d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout,\n",
        "#             activation=\"gelu\", batch_first=True, norm_first=True\n",
        "#         )\n",
        "#         self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "#         # Latent Space Projection\n",
        "#         self.latent_proj = nn.Linear(d_model, latent_dim)\n",
        "\n",
        "#         # Transformer Decoder\n",
        "#         decoder_layer = nn.TransformerDecoderLayer(\n",
        "#             d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout,\n",
        "#             activation=\"gelu\", batch_first=True, norm_first=True\n",
        "#         )\n",
        "#         self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "\n",
        "#         # Output Projection\n",
        "#         self.output_proj = nn.Linear(d_model, 1)\n",
        "\n",
        "#         # Residual Layer Norm\n",
        "#         self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # x: [batch_size, num_features=1, time_steps]\n",
        "#         x = x.permute(0, 2, 1)  # [batch_size, time_steps, num_features]\n",
        "#         x = self.input_proj(x)\n",
        "#         x = self.pos_encoder(x)\n",
        "\n",
        "#         # Encoder Forward Pass\n",
        "#         enc_output = self.encoder(x)\n",
        "#         enc_output = self.layer_norm(enc_output)\n",
        "\n",
        "#         # Latent Space Representation\n",
        "#         latent = self.latent_proj(enc_output)\n",
        "\n",
        "#         # Decoder Forward Pass (Reconstruction)\n",
        "#         dec_output = self.decoder(enc_output, enc_output)  # Using encoded input as target for reconstruction\n",
        "#         dec_output = self.layer_norm(dec_output)\n",
        "\n",
        "#         # Output Projection\n",
        "#         out = self.output_proj(dec_output)  # Shape: [batch_size, time_steps, 1]\n",
        "#         out = out.permute(0, 2, 1)  # Back to [batch_size, 1, time_steps]\n",
        "\n",
        "#         return out, latent  # Returning both the reconstruction and latent space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Qwx3UwHAiCmJ"
      },
      "outputs": [],
      "source": [
        "# class TransformerAutoencoder(nn.Module):\n",
        "#     def __init__(self, time_steps, d_model=128, nhead=4, num_layers=3, latent_dim=64, dim_feedforward=256, dropout=0.1):\n",
        "#         super().__init__()\n",
        "#         self.input_proj = nn.Linear(1, d_model)  # Project feature dim (1) to d_model\n",
        "#         self.pos_encoder = PositionalEncoding(d_model, max_len=time_steps)\n",
        "\n",
        "#         # Transformer Encoder\n",
        "#         encoder_layer = nn.TransformerEncoderLayer(\n",
        "#             d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout,\n",
        "#             activation=\"gelu\", batch_first=True, norm_first=True\n",
        "#         )\n",
        "#         self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "#         # Latent Space Projection (Global Pooling to get a single vector)\n",
        "#         self.latent_proj = nn.Linear(d_model, latent_dim)  # Reduce d_model → latent_dim\n",
        "#         self.reverse_proj = nn.Linear(latent_dim, d_model)\n",
        "#         self.latent_norm = nn.LayerNorm(latent_dim)  # Normalize latent space\n",
        "\n",
        "#         # Transformer Decoder\n",
        "#         decoder_layer = nn.TransformerDecoderLayer(\n",
        "#             d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout,\n",
        "#             activation=\"gelu\", batch_first=True, norm_first=True\n",
        "#         )\n",
        "#         self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "\n",
        "#         # Output Projection\n",
        "#         self.output_proj = nn.Linear(d_model, 1)\n",
        "\n",
        "#         # Ensure normalization for correct dimensions\n",
        "#         self.encoder_norm = nn.LayerNorm(d_model)\n",
        "#         self.decoder_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         \"\"\"\n",
        "#         x: [batch_size, num_features=1, time_steps]\n",
        "#         \"\"\"\n",
        "#         x = x.permute(0, 2, 1)  # Convert to [batch_size, time_steps, num_features]\n",
        "#         x = self.input_proj(x)  # Project input to d_model\n",
        "#         print(x.shape)\n",
        "#         x = self.pos_encoder(x)\n",
        "#         print(x.shape)\n",
        "#         # Encoder Forward Pass\n",
        "#         enc_output = self.encoder(x)  # Shape: [batch_size, time_steps, d_model]\n",
        "#         print(enc_output.shape)\n",
        "#         enc_output = self.encoder_norm(enc_output)  # Apply LayerNorm\n",
        "#         print(enc_output.shape)\n",
        "#         # Global Mean Pooling to get a single latent vector\n",
        "#         latent = enc_output.mean(dim=1, keepdim=True)  # Shape: [batch_size, 1, d_model]\n",
        "#         print(latent.shape)\n",
        "#         latent = self.latent_proj(latent)  # Shape: [batch_size, 1, latent_dim]\n",
        "#         print(latent.shape)\n",
        "#         latent = self.latent_norm(latent)  # Normalize latent space\n",
        "#         print(latent.shape)\n",
        "#         latent = self.reverse_proj(latent) # TODO: rename this variable because you have to return the latent representation which is from the last layer\n",
        "#         print(latent.shape)\n",
        "#         # Expand latent representation back to sequence length\n",
        "#         repeated_latent = latent.repeat(1, x.shape[1], 1)  # Shape: [batch_size, time_steps, latent_dim] # TODO: break symmetry by applying positional encoding here when repeating\n",
        "#         print(repeated_latent.shape)\n",
        "#         # Decoder Forward Pass\n",
        "#         dec_output = self.decoder(repeated_latent, enc_output)  # Shape: [batch_size, time_steps, d_model] # TODO: this is cheating because it is using the enc_output, which is before the bottleneck of the latent dimension\n",
        "#         print(dec_output.shape)\n",
        "#         dec_output = self.decoder_norm(dec_output)  # Apply LayerNorm\n",
        "#         print(dec_output.shape)\n",
        "#         # Output Projection\n",
        "#         out = self.output_proj(dec_output)  # Shape: [batch_size, time_steps, 1]\n",
        "#         out = out.permute(0, 2, 1)  # Back to [batch_size, 1, time_steps]\n",
        "\n",
        "#         return out, latent.squeeze(1)  # Returning [batch, 1, seq_len] & [batch, latent_dim]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerAutoencoder(nn.Module):\n",
        "    def __init__(self, time_steps, d_model=128, nhead=4, num_layers=3, latent_dim=64, dim_feedforward=256, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(1, d_model)  # Project feature dim (1) to d_model\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len=time_steps)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout,\n",
        "            activation=\"gelu\", batch_first=True, norm_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Latent Space Projection (Global Pooling to get a single vector)\n",
        "        self.latent_proj = nn.Linear(d_model, latent_dim)  # Reduce d_model → latent_dim\n",
        "        self.latent_norm = nn.LayerNorm(latent_dim)  # Normalize latent space\n",
        "\n",
        "        # Reverse Projection for Decoder Input\n",
        "        self.mult_latent = 10\n",
        "        self.reverse_proj = nn.Linear(latent_dim * self.mult_latent, d_model)\n",
        "\n",
        "        # Transformer Decoder\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout,\n",
        "            activation=\"gelu\", batch_first=True, norm_first=True\n",
        "        )\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Output Projection\n",
        "        self.output_proj = nn.Linear(d_model, 1)\n",
        "\n",
        "        # Normalization layers\n",
        "        self.encoder_norm = nn.LayerNorm(d_model)\n",
        "        self.decoder_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: [batch_size, num_features=1, time_steps]\n",
        "        \"\"\"\n",
        "        # Prepare input: [batch, time_steps, features]\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.input_proj(x)  # Project input to d_model\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        # Encoder Forward Pass\n",
        "        enc_output = self.encoder(x)  # [batch, time_steps, d_model]\n",
        "        enc_output = self.encoder_norm(enc_output)\n",
        "\n",
        "        # Global Mean Pooling to obtain a latent vector\n",
        "        # latent = enc_output.mean(dim=1, keepdim=True)  # [batch, 1, d_model] # TODO: instead of taking the mean across the time dimension, take the first 10 entries of the time dimension, and then reshape after the next latent_proj to get [batch, 1, latent_dim * 10]\n",
        "        latent = enc_output[:,:self.mult_latent,:]\n",
        "        # Project to latent space and normalize → this is the true latent representation\n",
        "        latent_code = self.latent_proj(latent)       # [batch, 10, latent_dim]\n",
        "        latent_code = self.latent_norm(latent_code)\n",
        "        latent_result = latent_code.view(latent_code.shape[0], -1)\n",
        "\n",
        "        # Prepare input for the decoder: use reverse projection on the latent_code\n",
        "        decoder_input = self.reverse_proj(latent_code)  # [batch, 10, d_model]\n",
        "        \n",
        "        # Expand decoder input back to sequence length\n",
        "        repeated_decoder_input = decoder_input.repeat(1, math.ceil(x.shape[1] / self.mult_latent), 1)\n",
        "        repeated_decoder_input = repeated_decoder_input[:,:x.shape[1],:] # [batch, time_steps, d_model]\n",
        "        # Break symmetry: apply positional encoding to the repeated latent representation\n",
        "        repeated_decoder_input = self.pos_encoder(repeated_decoder_input)\n",
        "        \n",
        "        # Decoder Forward Pass\n",
        "        # Instead of using enc_output as memory (which comes before the bottleneck), we now use\n",
        "        # repeated_decoder_input for both target and memory so the decoder solely relies on the bottleneck.\n",
        "        dec_output = self.decoder(repeated_decoder_input, repeated_decoder_input)  # [batch, time_steps, d_model]\n",
        "        dec_output = self.decoder_norm(dec_output)\n",
        "        \n",
        "        # Output Projection\n",
        "        out = self.output_proj(dec_output)  # [batch, time_steps, 1]\n",
        "        out = out.permute(0, 2, 1)  # Convert back to [batch, 1, time_steps]\n",
        "\n",
        "        # Return both the reconstructed output and the latent representation (squeezed to remove the time dim)\n",
        "        return out, latent_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WbPE4VkVNJU",
        "outputId": "4cd76a01-6e07-407b-d25c-71ca84bcee01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAunPm3IVvn6",
        "outputId": "7c907ff9-2fb5-4050-ed85-c2d0676f13a7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/lb/7g293m4d46dd0kc7ycgb9_c80000gn/T/ipykernel_12960/582984366.py:1: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  rus1000 = pd.read_csv(\"RUSSELL_1000_5y.csv\")\n"
          ]
        }
      ],
      "source": [
        "rus1000 = pd.read_csv(\"RUSSELL_1000_5y.csv\")\n",
        "spy500 = pd.read_csv(\"SPY_500_5y.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Erw92nReV2M6"
      },
      "outputs": [],
      "source": [
        "X = spy500.iloc[2:,1:].dropna(axis=1).T.astype(float)\n",
        "X = X / X.iloc[:,0].values.reshape(488, 1)\n",
        "X_t, X_v = train_test_split(X.values, test_size=0.3, random_state=0)\n",
        "X_t = X_t.reshape(X_t.shape[0], 1, X_t.shape[-1])\n",
        "X_v = X_v.reshape(X_v.shape[0], 1, X_v.shape[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "z00rd8jBVFeJ"
      },
      "outputs": [],
      "source": [
        "model = TransformerAutoencoder(X_t.shape[2]).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(341, 1, 1257)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_t.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJo8eJ66jEHy",
        "outputId": "80950b52-0158-4c73-ea86-52be56e63353"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 1000, 128])\n",
            "torch.Size([32, 1000, 128])\n",
            "torch.Size([32, 1000, 128])\n",
            "torch.Size([32, 1000, 128])\n",
            "torch.Size([32, 1, 128])\n",
            "torch.Size([32, 1, 64])\n",
            "torch.Size([32, 1, 64])\n",
            "torch.Size([32, 1, 128])\n",
            "torch.Size([32, 1000, 128])\n"
          ]
        }
      ],
      "source": [
        "model(torch.zeros(32,1,1000).to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "P7ly3G0aWjBS"
      },
      "outputs": [],
      "source": [
        "train_dataset = TensorDataset(torch.tensor(X_t, dtype=torch.float32))\n",
        "val_dataset = TensorDataset(torch.tensor(X_v, dtype=torch.float32))\n",
        "all_dataset = TensorDataset(torch.tensor(X.values.reshape(X.shape[0], 1, X.shape[-1]), dtype=torch.float32))\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "all_loader = DataLoader(all_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kiTbWJ0LXHRu"
      },
      "outputs": [],
      "source": [
        "def train_autoencoder(model, train_loader, val_loader, num_epochs=50, lr=1e-4, device=\"cuda\"):\n",
        "    model.to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for (batch, ) in train_loader:\n",
        "            x = batch.to(device)  # Move batch to GPU/CPU\n",
        "            optimizer.zero_grad()\n",
        "            outputs, _ = model(x)  # Forward pass\n",
        "            loss = criterion(outputs, x)  # Compare to original input\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Compute average training loss\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for (batch, ) in val_loader:\n",
        "                x = batch.to(device)\n",
        "                outputs, _ = model(x)\n",
        "                loss = criterion(outputs, x)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        # Print epoch losses\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GAkfXX9dXlZL"
      },
      "outputs": [],
      "source": [
        "# model = train_autoencoder(model, train_loader, val_loader, num_epochs=50, lr=1e-4, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3HbJ5KNFYJRq"
      },
      "outputs": [],
      "source": [
        "# X_temp = next(iter(val_loader))[0].to(device)\n",
        "# outputs, latent = model(X_temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IOQKZ8gaaV0K"
      },
      "outputs": [],
      "source": [
        "# torch.nn.CosineSimilarity(dim=2)(X_temp, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "o2MjPkcHauKU"
      },
      "outputs": [],
      "source": [
        "# torch.linalg.norm(X_temp), torch.linalg.norm(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3XDfoqMbVF8",
        "outputId": "89a7e43a-fdb6-4f7b-88f7-dee124aa0840"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-12-3b6ab60d012f>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"model.pth\"))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.save(model.state_dict(), \"model.pth\")\n",
        "model.load_state_dict(torch.load(\"model.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "AEmJi7CcdI92"
      },
      "outputs": [],
      "source": [
        "# X_t.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "WubdjEG4c3m8",
        "outputId": "365abe8a-09d7-42ec-e6d1-2500102dcf91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 1257, 64])\n",
            "torch.Size([32, 1257, 64])\n",
            "torch.Size([32, 1257, 64])\n",
            "torch.Size([32, 1257, 64])\n",
            "torch.Size([32, 1257, 64])\n",
            "torch.Size([32, 1257, 64])\n",
            "torch.Size([32, 1257, 64])\n",
            "torch.Size([32, 1257, 64])\n",
            "torch.Size([32, 1257, 64])\n",
            "torch.Size([32, 1257, 64])\n",
            "torch.Size([32, 1257, 64])\n",
            "torch.Size([32, 1257, 64])\n",
            "torch.Size([32, 1257, 64])\n",
            "torch.Size([32, 1257, 64])\n",
            "torch.Size([32, 1257, 64])\n",
            "torch.Size([8, 1257, 64])\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-e5e6823aea52>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mlatents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mlatents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "latents = []\n",
        "with torch.no_grad():\n",
        "    for (batch, ) in all_loader:\n",
        "        x = batch.to(device)\n",
        "        output, latent = model(x)\n",
        "        print(latent.shape)\n",
        "        latents.append(latent.cpu().detach().numpy())\n",
        "latents = np.concatenate(latents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9NHbGNbqcAiX"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtIea5cxgHLc",
        "outputId": "9332ef4a-1576-4b62-918c-a1571d5c8ef0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(488, 1257, 64)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "latents.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "6ZTVewd8cPe2",
        "outputId": "f7b1f4e4-d446-4fc9-b45f-05ce0a7aa7d9"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Found array with dim 3. PCA expected <= 2.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-0d147bdd9d5d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlatents_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0mC\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mordered\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m \u001b[0;34m'np.ascontiguousarray'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \"\"\"\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_is_centered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mU\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;31m# The copy will happen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;31m# later, only if needed, once the solver negotiation below is done.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m         X = validate_data(\n\u001b[0m\u001b[1;32m    506\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2944\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2945\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2946\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1099\u001b[0m             )\n\u001b[1;32m   1100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1102\u001b[0m                 \u001b[0;34m\"Found array with dim %d. %s expected <= 2.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m                 \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. PCA expected <= 2."
          ]
        }
      ],
      "source": [
        "pca = PCA(n_components=2)\n",
        "latents_transformed = pca.fit_transform(latents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asusQxKtcWBv",
        "outputId": "75ac9824-3fde-4e13-922b-aecad52dec8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(488, 1257, 64)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "latents.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgXIp75icXIB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "hacklytics",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
